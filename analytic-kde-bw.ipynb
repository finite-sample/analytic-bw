{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af72d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method       n   noise  ker   ISE±SD        Evals±SD\n",
      "grid      100   0.5   gauss  0.1843± 0.0317 50.0± 0.0\n",
      "golden    100   0.5   gauss  0.1818± 0.0308 22.0± 0.0\n",
      "newton    100   0.5   gauss  0.1863± 0.0310 12.0± 0.0\n",
      "silver    100   0.5   gauss  0.1906± 0.0233 1.0± 0.0\n",
      "grid      100   1.0   gauss  0.0694± 0.0166 50.0± 0.0\n",
      "golden    100   1.0   gauss  0.0674± 0.0119 22.0± 0.0\n",
      "newton    100   1.0   gauss  0.0710± 0.0159 12.0± 0.0\n",
      "silver    100   1.0   gauss  0.0750± 0.0132 1.0± 0.0\n",
      "grid      100   2.0   gauss  0.0118± 0.0094 50.0± 0.0\n",
      "golden    100   2.0   gauss  0.0118± 0.0094 22.0± 0.0\n",
      "newton    100   2.0   gauss  0.0121± 0.0098 12.0± 0.0\n",
      "silver    100   2.0   gauss  0.0115± 0.0051 1.0± 0.0\n",
      "grid      200   0.5   gauss  0.1860± 0.0273 50.0± 0.0\n",
      "golden    200   0.5   gauss  0.1846± 0.0252 22.0± 0.0\n",
      "newton    200   0.5   gauss  0.1882± 0.0275 12.0± 0.0\n",
      "silver    200   0.5   gauss  0.1889± 0.0229 1.0± 0.0\n",
      "grid      200   1.0   gauss  0.0685± 0.0162 50.0± 0.0\n",
      "golden    200   1.0   gauss  0.0664± 0.0160 22.0± 0.0\n",
      "newton    200   1.0   gauss  0.0690± 0.0163 12.0± 0.0\n",
      "silver    200   1.0   gauss  0.0686± 0.0119 1.0± 0.0\n",
      "grid      200   2.0   gauss  0.0072± 0.0026 50.0± 0.0\n",
      "golden    200   2.0   gauss  0.0072± 0.0025 22.0± 0.0\n",
      "newton    200   2.0   gauss  0.0073± 0.0028 9.2± 4.8\n",
      "silver    200   2.0   gauss  0.0094± 0.0030 1.0± 0.0\n",
      "grid      500   0.5   gauss  0.1938± 0.0173 50.0± 0.0\n",
      "golden    500   0.5   gauss  0.1941± 0.0172 22.0± 0.0\n",
      "newton    500   0.5   gauss  0.1952± 0.0174 12.0± 0.0\n",
      "silver    500   0.5   gauss  0.1991± 0.0148 1.0± 0.0\n",
      "grid      500   1.0   gauss  0.0761± 0.0088 50.0± 0.0\n",
      "golden    500   1.0   gauss  0.0753± 0.0094 22.0± 0.0\n",
      "newton    500   1.0   gauss  0.0766± 0.0089 12.0± 0.0\n",
      "silver    500   1.0   gauss  0.0773± 0.0086 1.0± 0.0\n",
      "grid      500   2.0   gauss  0.0083± 0.0031 50.0± 0.0\n",
      "golden    500   2.0   gauss  0.0083± 0.0031 22.0± 0.0\n",
      "newton    500   2.0   gauss  0.0083± 0.0032 6.5± 5.5\n",
      "silver    500   2.0   gauss  0.0092± 0.0026 1.0± 0.0\n",
      "grid      100   0.5   epan   0.1871± 0.0343 50.0± 0.0\n",
      "golden    100   0.5   epan   0.1815± 0.0334 22.0± 0.0\n",
      "newton    100   0.5   epan   0.1882± 0.0341 10.9± 3.3\n",
      "silver    100   0.5   epan   0.2149± 0.0259 1.0± 0.0\n",
      "grid      100   1.0   epan   0.0718± 0.0172 50.0± 0.0\n",
      "golden    100   1.0   epan   0.0683± 0.0153 22.0± 0.0\n",
      "newton    100   1.0   epan   0.0725± 0.0189 7.6± 5.4\n",
      "silver    100   1.0   epan   0.0901± 0.0143 1.0± 0.0\n",
      "grid      100   2.0   epan   0.0132± 0.0111 50.0± 0.0\n",
      "golden    100   2.0   epan   0.0096± 0.0050 22.0± 0.0\n",
      "newton    100   2.0   epan   0.0135± 0.0108 4.3± 5.0\n",
      "silver    100   2.0   epan   0.0186± 0.0067 1.0± 0.0\n",
      "grid      200   0.5   epan   0.1894± 0.0294 50.0± 0.0\n",
      "golden    200   0.5   epan   0.1863± 0.0256 22.0± 0.0\n",
      "newton    200   0.5   epan   0.1897± 0.0304 11.7± 1.5\n",
      "silver    200   0.5   epan   0.2063± 0.0248 1.0± 0.0\n",
      "grid      200   1.0   epan   0.0698± 0.0174 50.0± 0.0\n",
      "golden    200   1.0   epan   0.0670± 0.0171 22.0± 0.0\n",
      "newton    200   1.0   epan   0.0718± 0.0184 8.6± 5.0\n",
      "silver    200   1.0   epan   0.0798± 0.0140 1.0± 0.0\n",
      "grid      200   2.0   epan   0.0077± 0.0026 50.0± 0.0\n",
      "golden    200   2.0   epan   0.0074± 0.0026 22.0± 0.0\n",
      "newton    200   2.0   epan   0.0077± 0.0026 1.6± 2.4\n",
      "silver    200   2.0   epan   0.0132± 0.0035 1.0± 0.0\n",
      "grid      500   0.5   epan   0.1969± 0.0178 50.0± 0.0\n",
      "golden    500   0.5   epan   0.1930± 0.0155 22.0± 0.0\n",
      "newton    500   0.5   epan   0.1957± 0.0182 10.1± 4.0\n",
      "silver    500   0.5   epan   0.2105± 0.0155 1.0± 0.0\n",
      "grid      500   1.0   epan   0.0758± 0.0100 50.0± 0.0\n",
      "golden    500   1.0   epan   0.0743± 0.0097 22.0± 0.0\n",
      "newton    500   1.0   epan   0.0764± 0.0102 6.5± 5.5\n",
      "silver    500   1.0   epan   0.0847± 0.0092 1.0± 0.0\n",
      "grid      500   2.0   epan   0.0085± 0.0035 50.0± 0.0\n",
      "golden    500   2.0   epan   0.0083± 0.0033 22.0± 0.0\n",
      "newton    500   2.0   epan   0.0084± 0.0032 2.1± 3.3\n",
      "silver    500   2.0   epan   0.0116± 0.0031 1.0± 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from typing import Callable, Tuple, Dict\n",
    "\n",
    "\"\"\"============================================================================\n",
    "Analytic‑Hessian bandwidth selection for univariate KDE\n",
    "============================================================================\n",
    "Benchmarks (per kernel) the *same* finite‑sample Least‑Squares‑CV (LSCV)\n",
    "surface with four optimisers:\n",
    "    • grid search      –   50 log‑spaced points (reference optimum)\n",
    "    • golden section   –   derivative‑free line search\n",
    "    • Newton–Armijo    –   analytic grad & Hess (this work)\n",
    "    • Silverman        –   closed‑form plug‑in rule\n",
    "\n",
    "Two kernels are supported – **Gaussian** (classic) and **Epanechnikov** – with\n",
    "closed‑form convolution, gradient and Hessian for both.\n",
    "Outputs mean ± s.d. Integrated‑Squared‑Error (ISE) and evaluation counts across\n",
    "*R* Monte‑Carlo replicates.\n",
    "============================================================================\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  Kernel primitives & helper polynomials\n",
    "# ---------------------------------------------------------------------------\n",
    "SQRT_2PI = np.sqrt(2 * np.pi)\n",
    "_pairwise_sq = lambda x: (x[:, None] - x[None, :]) ** 2   # n×n squared dist\n",
    "\n",
    "# Helper that robustly handles a scalar *or* array expression ----------------\n",
    "\n",
    "def _poly_mask(u: np.ndarray, mask: np.ndarray, expr):\n",
    "    \"\"\"Return `expr` on `mask`, 0 elsewhere. `expr` may be scalar or array.\"\"\"\n",
    "    out = np.zeros_like(u, dtype=float)\n",
    "    if np.isscalar(expr):\n",
    "        out[mask] = expr\n",
    "    else:  # array‑valued polynomial already evaluated element‑wise\n",
    "        out[mask] = expr[mask]\n",
    "    return out\n",
    "\n",
    "# —— Gaussian ————————————————————————————————————————————\n",
    "K_gauss      = lambda u: np.exp(-0.5 * u * u) / SQRT_2PI\n",
    "K_gauss_p    = lambda u: -u * K_gauss(u)                   # derivative\n",
    "K_gauss_pp   = lambda u: (u * u - 1) * K_gauss(u)          # 2nd‑derivative\n",
    "K2_gauss     = lambda u: np.exp(-0.25 * u * u) / np.sqrt(4 * np.pi)  # conv.\n",
    "K2_gauss_p   = lambda u: -0.5 * u * K2_gauss(u)\n",
    "K2_gauss_pp  = lambda u: (0.25 * u * u - 0.5) * K2_gauss(u)\n",
    "\n",
    "# —— Epanechnikov —————————————————————————————————————————\n",
    "#   K(u) = 0.75 (1-u²) 1{|u|≤1};   convolution & derivatives are piecewise\n",
    "_absu = lambda u: np.abs(u)\n",
    "K_epan = lambda u: _poly_mask(u, _absu(u) <= 1, 0.75 * (1 - u * u))\n",
    "K_epan_p  = lambda u: _poly_mask(u, _absu(u) <= 1, -1.5 * u)\n",
    "K_epan_pp = lambda u: _poly_mask(u, _absu(u) <= 1, -1.5)\n",
    "\n",
    "# Convolution K*K  (valid for |u|≤2)\n",
    "K2_epan = lambda u: _poly_mask(\n",
    "    u,\n",
    "    _absu(u) <= 2,\n",
    "    0.6 - 0.75 * _absu(u) ** 2 + 0.375 * _absu(u) ** 3 - 0.01875 * _absu(u) ** 5,\n",
    ")\n",
    "K2_epan_p = lambda u: _poly_mask(\n",
    "    u,\n",
    "    _absu(u) <= 2,\n",
    "    np.sign(u) * (-0.09375 * _absu(u) ** 4 + 1.125 * _absu(u) ** 2 - 1.5 * _absu(u)),\n",
    ")\n",
    "K2_epan_pp = lambda u: _poly_mask(\n",
    "    u,\n",
    "    _absu(u) <= 2,\n",
    "    -0.375 * _absu(u) ** 3 + 2.25 * _absu(u) - 1.5,\n",
    ")\n",
    "\n",
    "KERNELS: Dict[str, Tuple[Callable, Callable, Callable, Callable, Callable, Callable]] = {\n",
    "    \"gauss\": (K_gauss, K_gauss_p, K_gauss_pp, K2_gauss, K2_gauss_p, K2_gauss_pp),\n",
    "    \"epan\" : (K_epan , K_epan_p , K_epan_pp , K2_epan , K2_epan_p , K2_epan_pp),\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  Analytic LSCV score / gradient / Hessian  (generic kernel)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def lscv_generic(x: np.ndarray, h: float, kernel: str):\n",
    "    \"\"\"Return (LSCV, ∂, ∂²) at bandwidth *h* for the chosen kernel.\"\"\"\n",
    "    K, Kp, Kpp, K2, K2p, K2pp = KERNELS[kernel]\n",
    "    n = len(x)\n",
    "    u = (x[:, None] - x[None, :]) / h\n",
    "\n",
    "    # ----- score -----\n",
    "    term1 = K2(u).sum() / (n ** 2 * h)\n",
    "    term2 = (K(u).sum() - np.sum(np.diag(K(u)))) / (n * (n - 1) * h)\n",
    "    score = term1 - 2 * term2\n",
    "\n",
    "    # ----- gradient -----\n",
    "    S_F = (K2(u) + u * K2p(u)).sum()\n",
    "    S_K = (K(u) + u * Kp(u)).sum() - 0.0  # i=j derivatives already excluded\n",
    "    grad = -S_F / (n ** 2 * h ** 2) + 2 * S_K / (n * (n - 1) * h ** 2)\n",
    "\n",
    "    # ----- Hessian -----\n",
    "    S_F2 = (2 * K2p(u) + u * K2pp(u)).sum()\n",
    "    S_K2 = (2 * Kp(u) + u * Kpp(u)).sum()\n",
    "    hess = 2 * S_F / (n ** 2 * h ** 3) - S_F2 / (n ** 2 * h ** 2)\n",
    "    hess += -4 * S_K / (n * (n - 1) * h ** 3) + 2 * S_K2 / (n * (n - 1) * h ** 2)\n",
    "    return score, grad, hess\n",
    "\n",
    "# Fast wrappers -------------------------------------------------------------\n",
    "lscv_gauss = lambda x, h: lscv_generic(x, h, \"gauss\")\n",
    "lscv_epan  = lambda x, h: lscv_generic(x, h, \"epan\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.  Newton–Armijo optimiser (scalar bandwidth)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def newton_opt(\n",
    "    x: np.ndarray,\n",
    "    h0: float,\n",
    "    score_grad_hess,\n",
    "    tol: float = 1e-5,\n",
    "    max_iter: int = 12,\n",
    "):\n",
    "    h, evals = h0, 0\n",
    "    for _ in range(max_iter):\n",
    "        f, g, H = score_grad_hess(x, h); evals += 1\n",
    "        if abs(g) < tol:\n",
    "            break\n",
    "        step = -g / H if (H > 0 and np.isfinite(H)) else -0.25 * g\n",
    "        if abs(step) / h < 1e-3:\n",
    "            break\n",
    "        # Armijo back‑tracking (cheap, no new Jacobians)\n",
    "        for _ in range(10):\n",
    "            h_new = max(h + step, 1e-6)\n",
    "            if score_grad_hess(x, h_new)[0] < f:\n",
    "                h = h_new; break\n",
    "            step *= 0.5\n",
    "    return h, evals\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.  Other optimisers & helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def golden_section(obj, a: float, b: float, tol: float = 1e-3):\n",
    "    gr = (np.sqrt(5) - 1) / 2\n",
    "    c, d = b - gr * (b - a), a + gr * (b - a)\n",
    "    fc, fd, evals = obj(c), obj(d), 2\n",
    "    while b - a > tol:\n",
    "        if fc < fd:\n",
    "            b, d, fd = d, c, fc\n",
    "            c = b - gr * (b - a)\n",
    "            fc = obj(c)\n",
    "        else:\n",
    "            a, c, fc = c, d, fd\n",
    "            d = a + gr * (b - a)\n",
    "            fd = obj(d)\n",
    "        evals += 1\n",
    "    return (a + b) / 2, evals\n",
    "\n",
    "def silverman_bandwidth(x: np.ndarray):\n",
    "    n = len(x)\n",
    "    sigma = np.std(x, ddof=1)\n",
    "    iqr = np.subtract(*np.percentile(x, [75, 25]))\n",
    "    sigma = min(sigma, iqr / 1.349)\n",
    "    return 0.9 * sigma * n ** (-1 / 5)\n",
    "\n",
    "# Newton initialised from coarse grid --------------------------------------\n",
    "\n",
    "def bandwidth_newton(x: np.ndarray, kernel: str):\n",
    "    grid = np.logspace(-1, 1, 20)\n",
    "    scorefun = lscv_gauss if kernel == \"gauss\" else lscv_epan\n",
    "    scores = [scorefun(x, h)[0] for h in grid]\n",
    "    h0 = grid[int(np.argmin(scores))]\n",
    "    return newton_opt(x, h0, scorefun)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.  ISE benchmark utilities\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def true_pdf(z: np.ndarray, noise: float):\n",
    "    return 0.5 * norm.pdf(z, -2, np.sqrt(0.5 ** 2 + noise ** 2)) + 0.5 * norm.pdf(z, 2, np.sqrt(1.0 ** 2 + noise ** 2))\n",
    "\n",
    "def ise_estimate(x: np.ndarray, h: float, noise: float, kernel: str):\n",
    "    K, *_ = KERNELS[kernel]\n",
    "    zz = np.linspace(-8, 8, 601)\n",
    "    est = np.array([K((z - x) / h).mean() / h for z in zz])\n",
    "    return np.trapz((est - true_pdf(zz, noise)) ** 2, zz)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6.  Monte‑Carlo benchmark (Gaussian & Epanechnikov)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def benchmark_kde(n: int, noise: float, kernel: str, R: int = 20):\n",
    "    rng = np.random.default_rng(int(n * 123 + noise * 10))\n",
    "    grid_h = np.logspace(-1, 1, 50)\n",
    "    scorefun = lscv_gauss if kernel == \"gauss\" else lscv_epan\n",
    "    methods = {m: ([], []) for m in [\"grid\", \"golden\", \"newton\", \"silver\"]}\n",
    "\n",
    "    for _ in range(R):\n",
    "        x = rng.normal(0, 1, n) + rng.normal(0, noise, n)\n",
    "\n",
    "        # Grid ---------------------------------\n",
    "        scores = [scorefun(x, h)[0] for h in grid_h]\n",
    "        h_g = grid_h[int(np.argmin(scores))]\n",
    "        methods[\"grid\"][0].append(ise_estimate(x, h_g, noise, kernel))\n",
    "        methods[\"grid\"][1].append(len(grid_h))\n",
    "\n",
    "        # Golden -------------------------------\n",
    "        obj = lambda h: scorefun(x, h)[0]\n",
    "        h_gold, eval_gold = golden_section(obj, grid_h[0], grid_h[-1])\n",
    "        methods[\"golden\"][0].append(ise_estimate(x, h_gold, noise, kernel))\n",
    "        methods[\"golden\"][1].append(eval_gold)\n",
    "\n",
    "        # Newton -------------------------------\n",
    "        h_new, eval_new = bandwidth_newton(x, kernel)\n",
    "        methods[\"newton\"][0].append(ise_estimate(x, h_new, noise, kernel))\n",
    "        methods[\"newton\"][1].append(eval_new)\n",
    "\n",
    "        # Silverman ----------------------------\n",
    "        h_sil = silverman_bandwidth(x)\n",
    "        methods[\"silver\"][0].append(ise_estimate(x, h_sil, noise, kernel))\n",
    "        methods[\"silver\"][1].append(1)\n",
    "\n",
    "    return methods\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 7.  Pretty‑print helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def summarise(arr):\n",
    "    return f\"{np.mean(arr): .4f}±{np.std(arr): .4f}\"\n",
    "\n",
    "def summarise_int(arr):\n",
    "    return f\"{np.mean(arr): .1f}±{np.std(arr): .1f}\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 8.  Main entry\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    ns, noises = [100, 200, 500], [0.5, 1.0, 2.0]\n",
    "    print(\"Method       n   noise  ker   ISE±SD        Evals±SD\")\n",
    "    for kernel in [\"gauss\", \"epan\"]:\n",
    "        for n in ns:\n",
    "            for noise in noises:\n",
    "                res = benchmark_kde(n, noise, kernel, R=20)\n",
    "                for m in [\"grid\", \"golden\", \"newton\", \"silver\"]:\n",
    "                    ise_str = summarise(res[m][0]); ev_str = summarise_int(res[m][1])\n",
    "                    print(f\"{m:<10}{n:<6}{noise:<6}{kernel:<6}{ise_str:<15}{ev_str}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01db648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
