{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e425634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- noise=0.1, n=200, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0009±0.0003    250.0±0.0\n",
      "Plug-in        0.0301±0.0017    5.0±0.0\n",
      "Newton         0.0014±0.0003    35.0±0.0\n",
      "AnalyticNewton 0.0009±0.0003    0.0±0.0\n",
      "Golden         0.0009±0.0003    75.0±0.0\n",
      "Bayes          0.0009±0.0003    75.0±0.0\n",
      "\n",
      "-- noise=0.1, n=200, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0009±0.0004    250.0±0.0\n",
      "Plug-in        0.0041±0.0008    5.0±0.0\n",
      "Newton         0.0041±0.0008    20.0±0.0\n",
      "AnalyticNewton 0.0011±0.0004    0.0±0.0\n",
      "Golden         0.0009±0.0004    75.0±0.0\n",
      "Bayes          0.0009±0.0004    75.0±0.0\n",
      "\n",
      "-- noise=0.1, n=500, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0005±0.0001    250.0±0.0\n",
      "Plug-in        0.0185±0.0007    5.0±0.0\n",
      "Newton         0.0005±0.0001    47.0±6.0\n",
      "AnalyticNewton 0.0005±0.0001    0.0±0.0\n",
      "Golden         0.0005±0.0001    75.0±0.0\n",
      "Bayes          0.0005±0.0001    75.0±0.0\n",
      "\n",
      "-- noise=0.1, n=500, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0005±0.0001    250.0±0.0\n",
      "Plug-in        0.0023±0.0002    5.0±0.0\n",
      "Newton         0.0023±0.0002    20.0±0.0\n",
      "AnalyticNewton 0.0005±0.0002    0.0±0.0\n",
      "Golden         0.0005±0.0001    75.0±0.0\n",
      "Bayes          0.0005±0.0001    75.0±0.0\n",
      "\n",
      "-- noise=0.1, n=1000, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0003±0.0001    250.0±0.0\n",
      "Plug-in        0.0126±0.0003    5.0±0.0\n",
      "Newton         0.0003±0.0001    56.0±7.3\n",
      "AnalyticNewton 0.0003±0.0001    0.0±0.0\n",
      "Golden         0.0003±0.0001    75.0±0.0\n",
      "Bayes          0.0003±0.0001    75.0±0.0\n",
      "\n",
      "-- noise=0.1, n=1000, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0003±0.0001    250.0±0.0\n",
      "Plug-in        0.0014±0.0001    5.0±0.0\n",
      "Newton         0.0014±0.0001    20.0±0.0\n",
      "AnalyticNewton 0.0003±0.0001    0.0±0.0\n",
      "Golden         0.0003±0.0001    75.0±0.0\n",
      "Bayes          0.0003±0.0001    75.0±0.0\n",
      "\n",
      "-- noise=0.2, n=200, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0026±0.0013    250.0±0.0\n",
      "Plug-in        0.0304±0.0035    5.0±0.0\n",
      "Newton         0.0057±0.0012    35.0±0.0\n",
      "AnalyticNewton 0.0026±0.0012    0.0±0.0\n",
      "Golden         0.0026±0.0012    75.0±0.0\n",
      "Bayes          0.0025±0.0012    75.0±0.0\n",
      "\n",
      "-- noise=0.2, n=200, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0028±0.0013    250.0±0.0\n",
      "Plug-in        0.0050±0.0019    5.0±0.0\n",
      "Newton         0.0050±0.0019    20.0±0.0\n",
      "AnalyticNewton 0.0028±0.0014    0.0±0.0\n",
      "Golden         0.0026±0.0012    75.0±0.0\n",
      "Bayes          0.0027±0.0013    75.0±0.0\n",
      "\n",
      "-- noise=0.2, n=500, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0014±0.0004    250.0±0.0\n",
      "Plug-in        0.0187±0.0013    5.0±0.0\n",
      "Newton         0.0014±0.0004    51.5±10.5\n",
      "AnalyticNewton 0.0014±0.0004    0.0±0.0\n",
      "Golden         0.0014±0.0004    75.0±0.0\n",
      "Bayes          0.0014±0.0004    75.0±0.0\n",
      "\n",
      "-- noise=0.2, n=500, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0014±0.0004    250.0±0.0\n",
      "Plug-in        0.0027±0.0005    5.0±0.0\n",
      "Newton         0.0027±0.0005    20.0±0.0\n",
      "AnalyticNewton 0.0014±0.0004    0.0±0.0\n",
      "Golden         0.0014±0.0004    75.0±0.0\n",
      "Bayes          0.0015±0.0004    75.0±0.0\n",
      "\n",
      "-- noise=0.2, n=1000, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0008±0.0002    250.0±0.0\n",
      "Plug-in        0.0126±0.0006    5.0±0.0\n",
      "Newton         0.0008±0.0002    48.5±8.1\n",
      "AnalyticNewton 0.0008±0.0002    0.0±0.0\n",
      "Golden         0.0008±0.0002    75.0±0.0\n",
      "Bayes          0.0008±0.0002    75.0±0.0\n",
      "\n",
      "-- noise=0.2, n=1000, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0008±0.0002    250.0±0.0\n",
      "Plug-in        0.0016±0.0002    5.0±0.0\n",
      "Newton         0.0016±0.0002    20.0±0.0\n",
      "AnalyticNewton 0.0008±0.0002    0.0±0.0\n",
      "Golden         0.0008±0.0002    75.0±0.0\n",
      "Bayes          0.0008±0.0002    75.0±0.0\n",
      "\n",
      "-- noise=0.5, n=200, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0108±0.0066    250.0±0.0\n",
      "Plug-in        0.0327±0.0094    5.0±0.0\n",
      "Newton         0.0182±0.0165    65.0±21.2\n",
      "AnalyticNewton 0.0106±0.0066    0.0±0.0\n",
      "Golden         0.0106±0.0066    75.0±0.0\n",
      "Bayes          0.0106±0.0067    75.0±0.0\n",
      "\n",
      "-- noise=0.5, n=200, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0109±0.0064    250.0±0.0\n",
      "Plug-in        0.0108±0.0071    5.0±0.0\n",
      "Newton         0.0198±0.0212    60.5±49.3\n",
      "AnalyticNewton 0.0111±0.0064    0.0±0.0\n",
      "Golden         0.0111±0.0064    75.0±0.0\n",
      "Bayes          0.0122±0.0064    75.0±0.0\n",
      "\n",
      "-- noise=0.5, n=500, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0056±0.0020    250.0±0.0\n",
      "Plug-in        0.0198±0.0034    5.0±0.0\n",
      "Newton         0.0055±0.0020    66.5±8.1\n",
      "AnalyticNewton 0.0055±0.0020    0.0±0.0\n",
      "Golden         0.0055±0.0020    75.0±0.0\n",
      "Bayes          0.0055±0.0021    75.0±0.0\n",
      "\n",
      "-- noise=0.5, n=500, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0056±0.0022    250.0±0.0\n",
      "Plug-in        0.0055±0.0020    5.0±0.0\n",
      "Newton         0.0081±0.0089    50.0±36.1\n",
      "AnalyticNewton 0.0057±0.0021    0.0±0.0\n",
      "Golden         0.0057±0.0021    75.0±0.0\n",
      "Bayes          0.0061±0.0024    75.0±0.0\n",
      "\n",
      "-- noise=0.5, n=1000, kernel=gaussian --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0034±0.0010    250.0±0.0\n",
      "Plug-in        0.0130±0.0016    5.0±0.0\n",
      "Newton         0.0036±0.0015    51.5±10.5\n",
      "AnalyticNewton 0.0032±0.0007    0.0±0.0\n",
      "Golden         0.0032±0.0007    75.0±0.0\n",
      "Bayes          0.0033±0.0007    75.0±0.0\n",
      "\n",
      "-- noise=0.5, n=1000, kernel=epanechnikov --\n",
      "Method    MSE(mean±sd)      Evals(mean±sd)\n",
      "Grid           0.0033±0.0008    250.0±0.0\n",
      "Plug-in        0.0031±0.0005    5.0±0.0\n",
      "Newton         0.0054±0.0044    39.5±39.1\n",
      "AnalyticNewton 0.0033±0.0010    0.0±0.0\n",
      "Golden         0.0033±0.0009    75.0±0.0\n",
      "Bayes          0.0032±0.0008    75.0±0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
    "import math\n",
    "from scipy.special import erf  # for Gaussian CDF approximation\n",
    "import warnings\n",
    "\n",
    "# --- 1. Simulate data ---\n",
    "def simulate_data(n=200, noise_std=0.1, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    X = np.linspace(0, 1, n)\n",
    "    f = np.sin(2 * np.pi * X)\n",
    "    y = f + np.random.normal(0, noise_std, size=n)\n",
    "    return X, y, f\n",
    "\n",
    "# --- 2. Kernel functions ---\n",
    "def nw_predict(X_train, y_train, X_test, h, kernel='gaussian'):\n",
    "    X_train = np.asarray(X_train).ravel()\n",
    "    X_test  = np.asarray(X_test).ravel()\n",
    "    u = (X_test[:, None] - X_train[None, :]) / h\n",
    "    if kernel == 'gaussian':\n",
    "        w = np.exp(-0.5 * u**2) / (h * np.sqrt(2 * np.pi))\n",
    "    elif kernel == 'epanechnikov':\n",
    "        mask = np.abs(u) <= 1\n",
    "        w = np.zeros_like(u)\n",
    "        w[mask] = 0.75 * (1 - u[mask]**2) / h\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel '{kernel}'\")\n",
    "    # Handle zero-weight cases to avoid NaNs\n",
    "    numerator = (w * y_train).sum(axis=1)\n",
    "    denom = w.sum(axis=1)\n",
    "    zero_mask = denom == 0\n",
    "    if np.any(zero_mask):\n",
    "        denom[zero_mask] = len(X_train)\n",
    "        numerator[zero_mask] = y_train.sum()\n",
    "    return numerator / denom\n",
    "\n",
    "# --- 3. Cross-validation scorer ---\n",
    "class CVScorer:\n",
    "    def __init__(self, X, y, folds=5, kernel='gaussian'):\n",
    "        self.X = np.asarray(X).ravel()\n",
    "        self.y = np.asarray(y).ravel()\n",
    "        self.kf = KFold(n_splits=folds, shuffle=True, random_state=0)\n",
    "        self.kernel = kernel\n",
    "        self.evals = 0\n",
    "\n",
    "    def score(self, h):\n",
    "        mses = []\n",
    "        for train_idx, test_idx in self.kf.split(self.X):\n",
    "            Xtr, Xte = self.X[train_idx], self.X[test_idx]\n",
    "            ytr, yte = self.y[train_idx], self.y[test_idx]\n",
    "            ypred = nw_predict(Xtr, ytr, Xte, h, kernel=self.kernel)\n",
    "            mses.append(mean_squared_error(yte, ypred))\n",
    "            self.evals += 1\n",
    "        return np.mean(mses)\n",
    "\n",
    "# --- 4. Bandwidth selectors ---\n",
    "def grid_search_cv(scorer, h_grid):\n",
    "    best_h, best_score = None, np.inf\n",
    "    start = scorer.evals\n",
    "    for h in h_grid:\n",
    "        score = scorer.score(h)\n",
    "        if score < best_score:\n",
    "            best_score, best_h = score, h\n",
    "    return best_h, best_score, scorer.evals - start\n",
    "\n",
    "\n",
    "def plug_in_bandwidth(X):\n",
    "    sigma = np.std(np.asarray(X).ravel(), ddof=1)\n",
    "    n = len(X)\n",
    "    return 1.06 * sigma * n ** (-1/5)\n",
    "\n",
    "\n",
    "def newton_hessian_cv(scorer, h_init, h_min=1e-3, tol=1e-3, max_iter=10, eps=1e-4):\n",
    "    h = max(h_init, h_min)\n",
    "    start = scorer.evals\n",
    "    for _ in range(max_iter):\n",
    "        s0 = scorer.score(h)\n",
    "        s1 = scorer.score(h + eps)\n",
    "        s_1 = scorer.score(max(h - eps, h_min))\n",
    "        grad = (s1 - s_1) / (2 * eps)\n",
    "        hess = (s1 + s_1 - 2 * s0) / (eps ** 2)\n",
    "        if hess <= 0:\n",
    "            break\n",
    "        h_new = max(h_min, h - grad / hess)\n",
    "        if abs(h_new - h) < tol:\n",
    "            h = h_new\n",
    "            break\n",
    "        h = h_new\n",
    "    # final score and evals count\n",
    "    score = scorer.score(h)\n",
    "    evals = scorer.evals - start\n",
    "    return h, score, evals\n",
    "\n",
    "\n",
    "def analytic_newton_cv(scorer, h_init, h_min=1e-3, tol=1e-3, max_iter=10):\n",
    "    \"\"\"\n",
    "    Analytic Newton on LOOCV objective using closed-form CV objective with Armijo line search.\n",
    "    Supports Gaussian & Epanechnikov kernels analytically, counts zero CV evals.\n",
    "    \"\"\"\n",
    "    # Helper: compute analytic LOOCV objective, gradient, and Hessian without counting CV calls\n",
    "    def obj_grad_hess(h):\n",
    "        grad, hess, obj = 0.0, 0.0, 0.0\n",
    "        total = 0\n",
    "        for train_idx, test_idx in scorer.kf.split(scorer.X):\n",
    "            Xtr, Xte = scorer.X[train_idx], scorer.X[test_idx]\n",
    "            ytr, yte = scorer.y[train_idx], scorer.y[test_idx]\n",
    "            u = (Xte[:, None] - Xtr[None, :]) / h\n",
    "            if scorer.kernel == 'gaussian':\n",
    "                w = np.exp(-0.5 * u**2) / (h * np.sqrt(2 * np.pi))\n",
    "                d_w = w * ((u**2 - 1) / h)\n",
    "                dd_w = w * ((u**4 - 3*u**2 + 1) / (h**2))\n",
    "            else:  # epanechnikov\n",
    "                mask = np.abs(u) <= 1\n",
    "                w = np.zeros_like(u)\n",
    "                w[mask] = 0.75 * (1 - u[mask]**2) / h\n",
    "                d_w = np.zeros_like(u)\n",
    "                d_w[mask] = 0.75 * ((-1 + 3*u[mask]**2) / (h**2))\n",
    "                dd_w = np.zeros_like(u)\n",
    "                dd_w[mask] = 1.5 * ((1 - 6*u[mask]**2) / (h**3))\n",
    "            w_sum = w.sum(axis=1)\n",
    "            num = (w * ytr).sum(axis=1)\n",
    "            zero_mask = w_sum == 0\n",
    "            if np.any(zero_mask):\n",
    "                w_sum[zero_mask] = len(ytr)\n",
    "                num[zero_mask] = ytr.mean() * w_sum[zero_mask]\n",
    "            m = num / w_sum\n",
    "            residual = yte - m\n",
    "            obj += np.sum(residual**2)\n",
    "            d_num = (d_w * ytr).sum(axis=1)\n",
    "            dd_num = (dd_w * ytr).sum(axis=1)\n",
    "            d_den = d_w.sum(axis=1)\n",
    "            dd_den = dd_w.sum(axis=1)\n",
    "            dm = (d_num * w_sum - num * d_den) / (w_sum**2)\n",
    "            ddm = (\n",
    "                dd_num * w_sum - 2*d_num*d_den - num*dd_den\n",
    "                + 2*num*(d_den**2)/w_sum\n",
    "            ) / (w_sum**2)\n",
    "            dm[zero_mask] = 0\n",
    "            ddm[zero_mask] = 0\n",
    "            grad += -2 * np.sum(residual * dm)\n",
    "            hess += 2 * np.sum(dm**2 - residual * ddm)\n",
    "            total += len(yte)\n",
    "        return obj / total, grad, hess\n",
    "\n",
    "    h = max(h_init, h_min)\n",
    "    # initial evaluation\n",
    "    current_obj, _, _ = obj_grad_hess(h)\n",
    "    for _ in range(max_iter):\n",
    "        obj_val, grad, hess = obj_grad_hess(h)\n",
    "        # direction: Newton or gradient descent\n",
    "        if hess > 0 and np.isfinite(hess):\n",
    "            direction = -grad / hess\n",
    "        else:\n",
    "            direction = -grad\n",
    "        # Armijo line search\n",
    "        c1, tau = 1e-4, 0.5\n",
    "        alpha = 1.0\n",
    "        while alpha > 1e-4:\n",
    "            h_trial = max(h_min, h + alpha * direction)\n",
    "            new_obj, _, _ = obj_grad_hess(h_trial)\n",
    "            if new_obj <= obj_val + c1 * alpha * grad * direction:\n",
    "                break\n",
    "            alpha *= tau\n",
    "        h_new = h_trial\n",
    "        if abs(h_new - h) < tol:\n",
    "            h = h_new\n",
    "            break\n",
    "        h, current_obj = h_new, new_obj\n",
    "    # returns h and zero CV evals\n",
    "    return h, 0\n",
    "\n",
    "def golden_section_cv(scorer, a, b, tol=1e-3, max_iter=20):\n",
    "    phi = (1 + np.sqrt(5)) / 2\n",
    "    start = scorer.evals\n",
    "    c, d = b - (b - a) / phi, a + (b - a) / phi\n",
    "    f_c, f_d = scorer.score(c), scorer.score(d)\n",
    "    for _ in range(max_iter):\n",
    "        if abs(b - a) < tol:\n",
    "            break\n",
    "        if f_c < f_d:\n",
    "            b, f_d = d, f_c\n",
    "            d = c\n",
    "            c = b - (b - a) / phi\n",
    "            f_c = scorer.score(c)\n",
    "        else:\n",
    "            a, f_c = c, f_d\n",
    "            c = d\n",
    "            d = a + (b - a) / phi\n",
    "            f_d = scorer.score(d)\n",
    "    h = (a + b) / 2\n",
    "    return h, scorer.evals - start\n",
    "\n",
    "\n",
    "def bayes_opt_cv(scorer, a, b, init_points=5, n_iter=10):\n",
    "    \"\"\"\n",
    "    Bayesian optimization on CV score with automatic handling of ConvergenceWarning.\n",
    "    \"\"\"\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    start = scorer.evals\n",
    "    # Initial design points\n",
    "    Xs = np.linspace(a, b, init_points)\n",
    "    Ys = [scorer.score(x) for x in Xs]\n",
    "    for _ in range(n_iter):\n",
    "        # Fit GP with noise-level bounds, catch warnings\n",
    "        X_train = Xs.reshape(-1,1)\n",
    "        y_train = np.array(Ys)\n",
    "        base_kernel = C(1.0, (1e-3, 1e3)) * Matern(nu=2.5)\n",
    "        wk = WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1e6))\n",
    "        kernel = base_kernel + wk\n",
    "        attempts = 0\n",
    "        while attempts < 2:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "                gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True).fit(X_train, y_train)\n",
    "                # If noise_level hit lower bound, relax it\n",
    "                if any(issubclass(wi.category, ConvergenceWarning) for wi in w):\n",
    "                    lb, ub = wk.noise_level_bounds\n",
    "                    new_lb = max(lb / 10, 1e-8)\n",
    "                    wk = WhiteKernel(noise_level=wk.noise_level, noise_level_bounds=(new_lb, ub))\n",
    "                    kernel = base_kernel + wk\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "                break\n",
    "        # Acquisition: Expected Improvement\n",
    "        hs = np.linspace(a, b, 100)\n",
    "        mu, sigma = gp.predict(hs.reshape(-1,1), return_std=True)\n",
    "        best = np.min(Ys)\n",
    "        Z = (best - mu) / np.maximum(sigma, 1e-8)\n",
    "        cdf = 0.5 * (1 + erf(Z / math.sqrt(2)))\n",
    "        pdf = np.exp(-0.5 * Z**2) / math.sqrt(2*math.pi)\n",
    "        ei = (best - mu) * cdf + sigma * pdf\n",
    "        x_next = hs[np.argmax(ei)]\n",
    "        Ys.append(scorer.score(x_next))\n",
    "        Xs = np.append(Xs, x_next)\n",
    "    best_idx = np.argmin(Ys)\n",
    "    return float(Xs[best_idx]), scorer.evals - start\n",
    "\n",
    "# --- 5. Robustness Simulation ---\n",
    "if __name__ == '__main__':\n",
    "    replicates = 10\n",
    "    noise_levels = [0.1, 0.2, 0.5]\n",
    "    sample_sizes = [200, 500, 1000]\n",
    "    kernels = ['gaussian', 'epanechnikov']\n",
    "    h_grid = np.linspace(0.01, 0.5, 50)\n",
    "\n",
    "    for noise in noise_levels:\n",
    "        for n in sample_sizes:\n",
    "            for kernel in kernels:\n",
    "                print(f\"\\n-- noise={noise}, n={n}, kernel={kernel} --\")\n",
    "                methods = ['Grid','Plug-in','Newton','AnalyticNewton','Golden','Bayes']\n",
    "                results = {m: [] for m in methods}\n",
    "                for rep in range(replicates):\n",
    "                    X, y, f_true = simulate_data(n=n, noise_std=noise, random_state=rep)\n",
    "                    scorer = CVScorer(X, y, folds=5, kernel=kernel)\n",
    "                    # Grid\n",
    "                    h_g, _, e_g = grid_search_cv(scorer, h_grid)\n",
    "                    mse_g = mean_squared_error(f_true, nw_predict(X, y, X, h_g, kernel))\n",
    "                    results['Grid'].append((mse_g, e_g))\n",
    "                    # Plug-in\n",
    "                    h_p = plug_in_bandwidth(X)\n",
    "                    e_p_start = scorer.evals\n",
    "                    scorer.score(h_p)\n",
    "                    e_p = scorer.evals - e_p_start\n",
    "                    mse_p = mean_squared_error(f_true, nw_predict(X, y, X, h_p, kernel))\n",
    "                    results['Plug-in'].append((mse_p, e_p))\n",
    "                    # Newton (finite diff)\n",
    "                    h_n, _, e_n = newton_hessian_cv(scorer, h_init=h_p, h_min=h_grid[0])\n",
    "                    mse_n = mean_squared_error(f_true, nw_predict(X, y, X, h_n, kernel))\n",
    "                    results['Newton'].append((mse_n, e_n))\n",
    "                    # Analytic Newton\n",
    "                    h_a, e_a = analytic_newton_cv(scorer, h_init=h_p, h_min=h_grid[0])\n",
    "                    mse_a = mean_squared_error(f_true, nw_predict(X, y, X, h_a, kernel))\n",
    "                    results['AnalyticNewton'].append((mse_a, e_a))\n",
    "                    # Golden-section\n",
    "                    h_o, e_o = golden_section_cv(scorer, a=h_grid[0], b=h_grid[-1])\n",
    "                    mse_o = mean_squared_error(f_true, nw_predict(X, y, X, h_o, kernel))\n",
    "                    results['Golden'].append((mse_o, e_o))\n",
    "                    # Bayesian Optimization\n",
    "                    h_b, e_b = bayes_opt_cv(scorer, a=h_grid[0], b=h_grid[-1], init_points=5, n_iter=10)\n",
    "                    mse_b = mean_squared_error(f_true, nw_predict(X, y, X, h_b, kernel))\n",
    "                    results['Bayes'].append((mse_b, e_b))\n",
    "                # summary\n",
    "                print(\"Method    MSE(mean±sd)      Evals(mean±sd)\")\n",
    "                for m in methods:\n",
    "                    mses = [v[0] for v in results[m]]\n",
    "                    evs  = [v[1] for v in results[m]]\n",
    "                    print(f\"{m:<15}{np.mean(mses):.4f}±{np.std(mses):.4f}    {np.mean(evs):.1f}±{np.std(evs):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33dfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
